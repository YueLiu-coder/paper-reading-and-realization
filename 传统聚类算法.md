## 传统聚类算法

### K-Means聚类

步骤：

1. 首先需要确定几个聚类**K**（cluster，也称为簇），并为他初始化一个各自的**聚类中心**（cluster centroids）。

   要确定聚类的数量，我们可以先快速看一看已有的数据点，并从中分辨出一些独特的数据

2. 其次，我们计算每个数据点到**质心的距离**来进行**分类**，它跟哪个聚类的质心更近，它就被分类到该聚类

3. 初始的中心并不是真正的中心，质心应满足：**聚类里的每个点到它的欧式距离平方和最小**

   因此根据这些被初步分类完毕的数据点，我们重新计算每一个聚类中所有向量的平均值，作为**新的质心**

4. **重复**，迭代一定次数，直到质心的位置不再发生太大的变化。可以在第一步多初始化几次，然后选取一个比较合理的点



优点：

1. 速度非常快，因为只需要计算数据点到质心之间的距离

   算法复杂度为O(n)



缺点：

1. 必须一开始就决定数据集中包含**多少个聚类**
2. 聚类中心的选取是**随机的**



K-Medians和K-Means相关的另一种聚类算法，它用中值来计算质心。需要重新排序，**慢**，对**异常数据不敏感**





### Mean-Shift聚类

又称为**均值漂移算法**，是一种基于核密度估计的爬山算法，可用于聚类、图像分割、跟踪等

工作原理基于**质心**，目标是定位每个簇/类的质心，即先算出当前**点的偏移均值**，将该点移动到此偏移均值，然后作为新的起始点，继续移动，直到满足最终的条件



1. 随机选择一个**点c**，以它为圆心画一个**半径为r的圆**开始移动
2. 在每轮迭代中，算法会不断计算**圆心到质心**的**偏移均值**，然后整体向质心靠近
3. 当到达目标质心的时候，发现无论哪个方向偏移都找不到更多的数据点，则满足条件，退出



优点

1. 不需要定义聚类数量



缺点

1. 需要对高维球区域的半径r的定义，不同选择可能会产生高度不同的影响



### 具有噪声的基于密度的聚类方法DBSCAN



1. 首先，DBSCAN算法会以任何尚未访问过的任意起始数据点为核心点，并对该核心点进行扩充。这时我们给定一个半径/距离ε，任何和核心点的距离小于ε的点都是它的相邻点。
2. 如果核心点附近有足够数量的点，则开始聚类，且选中的核心点会成为该聚类的第一个点。如果附近的点不够，那算法会把它标记为噪声（之后这个噪声可能会成为簇中的一部分）。在这两种情形下，选中的点都会被标记为“已访问”。
3. 一旦聚类开始，核心点的相邻点，或者说以该点出发的所有密度相连的数据点（注意是密度相连）会被划分进同一聚类。然后我们再把这些新点作为核心点，向周围拓展ε，并把符合条件的点继续纳入这个聚类中。
4. 重复步骤2和3，直到附近没有可以扩充的数据点为止，即簇的ε邻域内所有点都已被标记为“已访问”。
5. 一旦我们完成了这个集群，算法又会开始检索未访问过的点，并发现更多的聚类和噪声。一旦数据检索完毕，每个点都被标记为属于一个聚类或是噪声。

与其他聚类算法相比，DBSCAN有一些很大的优势。首先，它不需要输入要划分的聚类个数。其次，不像mean-shift，即使数据点非常不同，它也会将它们纳入聚类中，DBSCAN能将异常值识别为噪声，这就意味着它可以在需要时输入过滤噪声的参数。第三，它对聚类的形状没有偏倚，可以找到任意大小和形状的簇。



缺点

1. 当聚类密度不同的时候，性能不如其他算法





### EM聚类

1. 首先，确定**聚类的数量K**，并随机初始化每个聚类的高斯分布参数
2. 根据每个聚类的高斯分布，计算数据点属于聚类的**概率**
3. 在这些概率的基础上，为高斯分布**重新计算一组参数**，使聚类内数据点的概率最大化
4. 重复2、3





### 层次聚类









reference:

[数据科学家需要了解的5种聚类算法](https://www.zhihu.com/search?type=content&q=%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95)